{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Day 3 Train NN v2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KTvJB4kTsVG",
        "colab_type": "text"
      },
      "source": [
        "# Train neural network\n",
        "\n",
        "- Define functions for training a neural network\n",
        "- Train a fully connect neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tf5XdzDfoaCN",
        "colab_type": "text"
      },
      "source": [
        "### Setup drive\n",
        "\n",
        "Run the following cell to mount your Drive onto Colab. Go to the given URL and once you login and copy and paste the authorization code, you should see \"drive\" pop up in the files tab on the left."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWNwff0qoeHm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "8ec39709-906a-4b26-bc2d-214722790858"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ2vK0Wkoh91",
        "colab_type": "text"
      },
      "source": [
        "Click the little triangle next to \"drive\" and navigate to the \"AI4All Chest X-Ray Project\" folder. Hover over the folder and click the 3 dots that appear on the right. Select \"copy path\" and replace `PASTE PATH HERE` with the path to your folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wmsbus7EolZs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# cd \"PASTE PATH HERE\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTEH61rgoys8",
        "colab_type": "text"
      },
      "source": [
        "### Import necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-MFSQW9TsVH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import nn, optim, Tensor\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "from utils.datahelper import get_random_image\n",
        "from utils.modelhelper import simple_train\n",
        "from utils.plotting import imshow_dataset, imshow_tensors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l-MiFYaTsVN",
        "colab_type": "text"
      },
      "source": [
        "### Setup paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41KCntJjTsVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_dataset = os.path.join('data')\n",
        "\n",
        "path_to_images = os.path.join(path_to_dataset, 'images')\n",
        "\n",
        "metadata = pd.read_csv(os.path.join(path_to_dataset, 'metadata_train.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xjVe9eg0HJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# setup device for gpu/cpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2y19y7cpc35",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess data\n",
        "\n",
        "Preprocess steps from the previous notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcFB-yFPTsVS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_MEAN = 0.544\n",
        "DATA_STD = 0.237\n",
        "\n",
        "# RESIZE_SIZE = 235\n",
        "# CROP_SIZE = 224 \n",
        "\n",
        "# smaller image size for quicker training\n",
        "RESIZE_SIZE = 50\n",
        "CROP_SIZE = 50 \n",
        "\n",
        "IM_SIZE = CROP_SIZE\n",
        "\n",
        "data_transforms = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.Resize(RESIZE_SIZE),\n",
        "        transforms.CenterCrop(IM_SIZE),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=DATA_MEAN, std=DATA_STD)])\n",
        "\n",
        "dataset = datasets.ImageFolder(path_to_images, transform=data_transforms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MW3JieQI_oe",
        "colab_type": "text"
      },
      "source": [
        "Run this block a few times to see the transform steps for different random images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTlcmgILkHDn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_notransform = datasets.ImageFolder(path_to_images, transform=None)\n",
        "im = get_random_image(dataset_notransform)\n",
        "\n",
        "# images for each step of the transformation\n",
        "im_greyscale = transforms.Grayscale()(im)\n",
        "im_resize = transforms.Resize(RESIZE_SIZE)(im_greyscale)\n",
        "im_crop = transforms.CenterCrop(IM_SIZE)(im_resize)\n",
        "im_tensor = transforms.ToTensor()(im_crop)\n",
        "im_normed = transforms.Normalize(mean=DATA_MEAN, std=DATA_STD)(im_tensor)\n",
        "\n",
        "# show images\n",
        "fig, ax = plt.subplots(1,5, figsize=(15,5))\n",
        "ax = ax.ravel()\n",
        "\n",
        "plt.gray()\n",
        "ax[0].imshow(im_greyscale)\n",
        "ax[1].imshow(im_resize)\n",
        "ax[2].imshow(im_crop)\n",
        "ax[3].imshow(np.squeeze(im_tensor))\n",
        "ax[4].imshow(np.squeeze(im_normed))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RullHijOmIho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tensor vs PIL image\n",
        "\n",
        "pil_size = im_crop.size\n",
        "tensor_size = im_tensor.shape\n",
        "\n",
        "print(pil_size)\n",
        "print(tensor_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOKkNZAjrPL8",
        "colab_type": "text"
      },
      "source": [
        "### Loading data with DataLoader\n",
        "Use DataLoader to define how we want to load in the data. [Read about the function here](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qj148dA1xaVd",
        "colab_type": "text"
      },
      "source": [
        "**Loading a batch of images**\n",
        "\n",
        "In this example, we define the DataLoader to load images from the dataset such that 5 images (`batch_size=5`) at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K-xyhHX5rceO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loader = DataLoader(dataset=dataset, batch_size=5)\n",
        "\n",
        "# get the next batch of images\n",
        "batch_dataset = next(iter(loader))\n",
        "\n",
        "# display this batch of image\n",
        "batch_images = batch_dataset[0]\n",
        "imshow_tensors(batch_images, n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJOjxDixdeI",
        "colab_type": "text"
      },
      "source": [
        "**Randomly shuffling when loading a batch of images**\n",
        "\n",
        "If we set `shuffle=True`, the images are randomly shuffled when we get the batches. This is useful if we want to mix up the training data so we don't train the model on all the Covid images first (and possible over-bias the model) before training on the No Finding images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REXlmqBlwqyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loader = DataLoader(dataset=dataset, shuffle=True, batch_size=5)\n",
        "\n",
        "# get the next batch of images\n",
        "batch_dataset = next(iter(loader))\n",
        "\n",
        "# display this batch of image\n",
        "batch_images = batch_dataset[0]\n",
        "imshow_tensors(batch_images, n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F58TfbbSxjQY",
        "colab_type": "text"
      },
      "source": [
        "**Transforms are applied \"just in time\"**\n",
        "\n",
        "The transforms we defined are applied to the images right before the DataLoader grabs a batch of images. This means that if there are random transformations, the same batch of images may be look differently every time they is called (for example, during different iterations of epochs)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKj-pvgqwvdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define transformations with added random affine step\n",
        "data_transforms_affine = transforms.Compose([\n",
        "        transforms.Grayscale(),\n",
        "        transforms.Resize(RESIZE_SIZE),\n",
        "        transforms.CenterCrop(IM_SIZE),\n",
        "        transforms.RandomAffine(20, translate=(0.2, 0.2)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=DATA_MEAN, std=DATA_STD)])\n",
        "\n",
        "dataset_affine = datasets.ImageFolder(path_to_images, transform=data_transforms_affine)\n",
        "\n",
        "loader = DataLoader(dataset=dataset_affine, shuffle=False, batch_size=5)\n",
        "\n",
        "# get the next batch of images\n",
        "batch_dataset = next(iter(loader))\n",
        "\n",
        "# display this batch of image\n",
        "batch_images = batch_dataset[0]\n",
        "imshow_tensors(batch_images, n=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55YzQuLiTsVh",
        "colab_type": "text"
      },
      "source": [
        "### Fully connected neural network example\n",
        "\n",
        "The nn.Sequential container is a simple method for defining a neural network in PyTorch. This method is approachable for getting started but is difficult to customize. \n",
        "\n",
        "For more custom and advanced networks, [check out the nn.Module class](https://pytorch.org/docs/master/generated/torch.nn.Module.html), which is the more standard approach for creating neural networks using PyTorch. Once you are comfortable with the code here, I would recommend creating a network using `nn.Module`. You can try defining the `predict` and `evaluate` methods directly in the class or try customizing the weight initalization function.\n",
        "\n",
        "**Print the NN model below**\n",
        "\n",
        " What's the layout of the network? How many parameters (weights) are we training between each layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmjXS45QTsVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = IM_SIZE * IM_SIZE ## INPUT SIZE ##\n",
        "\n",
        "model = nn.Sequential(nn.Flatten(),\n",
        "                      nn.Linear(input_size, 32),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(32, 8),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(8, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPEPzhhzyu6M",
        "colab_type": "text"
      },
      "source": [
        "**Train the network**\n",
        "\n",
        "We defined a function `simple_train` to do forward and backward propagation on the model. You will get a chance to expand on this function later in the notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OPGqvPzOTsVk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(dataset=dataset, batch_size=64)\n",
        "simple_train(model, train_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ch-aKci92sqD",
        "colab_type": "text"
      },
      "source": [
        "**How did this model do?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCtBstXnTsVV",
        "colab_type": "text"
      },
      "source": [
        "### Training and validation sets\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xbdn6Gairj52",
        "colab_type": "text"
      },
      "source": [
        "Remember that we don't want to train on all of the data. To evaluate the generalizability of the model (how well the model performs on unknown data), we need to split the data into training and validation datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD3437bsTsVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Write a function that calculates the training and validation set \n",
        "# sizes based on the ratio of data you want have in the training set. \n",
        "# For example, if the dataset has 100 images and you want 60% (0.6) in the training\n",
        "# set, the function should return (60, 40)\n",
        "\n",
        "def get_train_val_sizes(dataset, train_ratio):\n",
        "  ## WRITE CODE HERE ##\n",
        "  \n",
        "  return train_size, val_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7zRpYoL85s0r",
        "colab_type": "text"
      },
      "source": [
        "**Train and validation loaders**\n",
        "\n",
        "Note: Think about how you might want to set the parameters for DataLoader differently for the training and validation sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkT8WjrcTsVY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Use the random_split and DataLoader functions to get two DataLoaders,\n",
        "# one for training and one for validation\n",
        "\n",
        "train_ratio = # CODE #\n",
        "train_size, val_size = get_train_val_sizes(dataset, train_ratio)\n",
        "\n",
        "## WRITE CODE HERE ##"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDB9qNLgqdcN",
        "colab_type": "text"
      },
      "source": [
        "### Assess training progress\n",
        "\n",
        "Modify the training function to record the loss and accuracy so we can assess the training progress over iterations.\n",
        "\n",
        "**Calculate accuracy**\n",
        "\n",
        "Given a set of images, we can use the model to predict a list of labels. If we have 8 images, the model might output something like `predicted` in the code two blocks below. `predicted[0]` would be the predicted label for the first image, `predicted[1]` for the second, and so on. We can compare the predicted labels to the actual labels. If the labels are the same for an image, then the model has made a correct prediction. If the labels are different then the model has made an incorrect prediction. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_1aH4RLTsVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Write a function that calculates the accuracy (# correct / total) \n",
        "# given tensors (arrays) of predicted and actual labels. \n",
        "# Tip: check out the torch.mean() function\n",
        "\n",
        "def calc_accuracy(predicted, actual):\n",
        "    ## WRITE CODE HERE ##\n",
        "    \n",
        "    return accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SjowcQd77t2V",
        "colab_type": "text"
      },
      "source": [
        "Check your function by running this block. The output should be 0.625"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7l20w496v8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = Tensor([0,1,1,0,0,0,1,0])\n",
        "actual = Tensor([0,1,0,1,1,0,1,0])\n",
        "calc_accuracy(predicted, actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlNGqIuM8SaV",
        "colab_type": "text"
      },
      "source": [
        "**Make predictions**\n",
        "\n",
        "As you may remember, the output of the neural network is a set of probabilities for the class labels. If the model outputs `[0.6, 0.4]` for an image, this means that the model predicts the image to be a Covid case with 0.6 probability and a No Finding case with a 0.4 probability. To convert the probabilities into a class label prediction, we would take the label with the greater probability.  \n",
        "\n",
        "Note: In our case, the final output layer is actually a log probability (for reasons related to the loss function). Taking the max will still work since the log function is monotonically increasing. If you want to play around with this, you can convert the log probability to probability using `torch.exp()`\n",
        "\n",
        "Note 2: We are only taking the max here. However, you can imagine that a prediction with 0.9 probability is much stronger than a prediction with 0.55 probability. This is why we train the model using a loss function rather than the accuracy!\n",
        "\n",
        "You can check out what the model outputs look like using this code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czUmAATJIzJm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# get a batch of images (inputs)\n",
        "inputs, _ = next(iter(train_loader))\n",
        "\n",
        "# get the output from the model\n",
        "outputs = model(inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8wTnecmI4nA",
        "colab_type": "text"
      },
      "source": [
        "Write the `predict` function which will return the predicted labels for a model and a set of input images. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wl9g36vTsVc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Write a function that uses a trained neural network model to predict\n",
        "# labels for a list of input images\n",
        "# Tip: Check out the torch.max() function\n",
        "\n",
        "def predict(model, inputs):\n",
        "    ## WRITE CODE HERE ##\n",
        "    \n",
        "    return predicted"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLSPzxy3JdNL",
        "colab_type": "text"
      },
      "source": [
        "Run the code below to test your prediction function. You should get 0.6 as the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "henqmjGqJeiU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sets a manual random seed \n",
        "torch.manual_seed(8)\n",
        "\n",
        "# loader to test predict function\n",
        "testing_loader = DataLoader(dataset=dataset, shuffle=True, batch_size=10)\n",
        "\n",
        "# get first batch of images and class labels\n",
        "images, labels_actual = next(iter(testing_loader))\n",
        "\n",
        "# get predicted labels (from the prediction function above)\n",
        "labels_predicted = predict(model, images)\n",
        "\n",
        "# reset random seed\n",
        "_ = torch.seed()\n",
        "\n",
        "# calculate accuracy\n",
        "calc_accuracy(labels_predicted, labels_actual)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WsFI5jG8weC",
        "colab_type": "text"
      },
      "source": [
        "**Update the training function**\n",
        "\n",
        "The framework for training the model is provided in the `train` function below. \n",
        "\n",
        "In this function, we first define the loss function (`nn.NLLLoss`) and the optimization function (`optim.Adam`). We also load in the validation dataset so that we can compute the validation accuracy. \n",
        "\n",
        "The function then loops over each epoch and each batch. Remember that the epoch number is the number of times we loop over the entire dataset and each batch is a subset of the data for used one step of training. If we are running the model for 3 epochs and 10 batches / epoch, then the model will train a total of `3 * 10 = 30` steps.  \n",
        "\n",
        "The code inside the loops defines what happens during each training step: \n",
        "1. Get the next batch of data for training\n",
        "2. Forward propagation: Use the model to make predictions\n",
        "3. Calculate loss (error): How badly did the model do?\n",
        "4. Back propagation: Propagate the error and update the weights in a direction that would minimize the error.\n",
        "\n",
        "Calculate and record loss, training accuracy, and validation accuracy during model training so we can track the training progress. In the indicated parts of the function, add code so that the training function will keep track of the training loss, training accuracy, and validation accuracy during training. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHXKoJUQTsVg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# EXERCISE: Add code to the train function to print and return the training \n",
        "# accuracy, validation accuracy, and loss\n",
        "\n",
        "def train(model, train_loader, val_loader, epoch_num=1, lr=0.001):\n",
        "    '''Trains a neural network\n",
        "\n",
        "    Args:\n",
        "        model: torch model\n",
        "        train_loader (DataLoader): training data loader\n",
        "        val_loader (DataLoader): validation data loader\n",
        "        epoch_num (int): number of epochs\n",
        "        lr (float): learning rate\n",
        "\n",
        "    Return:\n",
        "        None (plots loss over iterations)\n",
        "    '''\n",
        "    \n",
        "    # define loss and optimization functions\n",
        "    criterion = nn.NLLLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # load validation set\n",
        "    inputs_val, targets_val = next(iter(val_loader))\n",
        "    \n",
        "    # create variables for tracking loss and accuracy values\n",
        "    \n",
        "    ### WRITE CODE HERE ###\n",
        "\n",
        "\n",
        "    for epoch in range(epoch_num):\n",
        "\n",
        "        for batch_num, data in enumerate(train_loader):\n",
        "\n",
        "            inputs, targets = data\n",
        "\n",
        "            # set parameter gradients to zero\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # calculate loss and update weights\n",
        "            loss = criterion(outputs, targets)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            ### WRITE CODE HERE\n",
        "\n",
        "\n",
        "\n",
        "            \n",
        "\n",
        "            ###\n",
        "\n",
        "            ### UPDATE LINE BELOW TO PRINT LOSS AND ACCURACY ###\n",
        "            print('[epoch {} batch {}]'.format(epoch, batch_num))\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "\n",
        "    # Returns training, validation accuracy, and loss\n",
        "    return train_accuracy_log, val_accuracy_log, loss_log"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTTdfVHF9_Nv",
        "colab_type": "text"
      },
      "source": [
        "Now try training a neural network model with the updated `train` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GELwGvrW1VpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy_log, val_accuracy_log, loss_log = train(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8YiDYEnTsVc",
        "colab_type": "text"
      },
      "source": [
        "### Modify neural network model\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1svlbJJlTsVm",
        "colab_type": "text"
      },
      "source": [
        "**Try a really small network**\n",
        "\n",
        "How did the performance change compared the the previous model?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUrJ5OeVTsVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = nn.Sequential(nn.Flatten(),\n",
        "                      nn.Linear(input_size, 4),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(4, 2),\n",
        "                      nn.LogSoftmax(dim=1))\n",
        "\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-xzISzVTsVo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_accuracy_log, val_accuracy_log, loss_log = train(model, train_loader, val_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD05cJGJ-th1",
        "colab_type": "text"
      },
      "source": [
        "**Modify network architecture and change hyperparameters**\n",
        "\n",
        "Try modifying the hyperparameters: number of layers, number of nodes, number of epochs, batch size. You can also modify the input image size. A larger input image will train slower but may give better results!\n",
        "\n",
        "If you are interested in other types of neural network layers, you can [read more here](https://pytorch.org/docs/master/nn.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySA63D4B1k1c",
        "colab_type": "text"
      },
      "source": [
        "**How did different architectures and hyperparameters change the learning performance?**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ofYWNL7eTsV7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}